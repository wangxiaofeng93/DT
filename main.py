import os
import argparse
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import models, transforms
from D2IFLN import D2IFLN
import torchvision.datasets as datasets

# è®¾ç½®å‚æ•°
parser = argparse.ArgumentParser()
# æ•°æ®é›†è®¾ç½®
parser.add_argument('--data_root', type=str, default='./data', help='æ•°æ®æ ¹ç›®å½•')
parser.add_argument('--dataset', type=str, default='PACS', help='æ•°æ®é›†: PACS, Office-Home, VLCS')
# è®­ç»ƒè®¾ç½®
parser.add_argument('--batch_size', type=int, default=30, help='æ‰¹å¤§å°')
parser.add_argument('--epochs', type=int, default=40, help='è®­ç»ƒè½®æ•°')
parser.add_argument('--lr', type=float, default=0.002, help='å­¦ä¹ ç‡')
parser.add_argument('--momentum', type=float, default=0.9, help='SGDåŠ¨é‡')
parser.add_argument('--weight_decay', type=float, default=5e-4, help='æƒé‡è¡°å‡')
# æ¨¡å‹è®¾ç½®
parser.add_argument('--backbone', type=str, default='resnet18', help='backbone: resnet18, resnet50')
parser.add_argument('--hidden_dim', type=int, default=128, help='éšè—å±‚ç»´åº¦')
parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
# æŸå¤±æƒé‡
parser.add_argument('--lambda_gcn', type=float, default=1.0, help='GCNæŸå¤±æƒé‡')
parser.add_argument('--lambda_cr', type=float, default=1.0, help='ä¸€è‡´æ€§æŸå¤±æƒé‡')
parser.add_argument('--lambda_adv', type=float, default=0.1, help='å¯¹æŠ—æŸå¤±æƒé‡')
parser.add_argument('--lambda_ent', type=float, default=1.0, help='ç†µæŸå¤±æƒé‡')
parser.add_argument('--lambda_mis', type=float, default=1.0, help='SDN MIæŸå¤±æƒé‡')
parser.add_argument('--lambda_dce', type=float, default=1.0, help='åŸŸåˆ†ç±»æŸå¤±æƒé‡')
parser.add_argument('--lambda_mid', type=float, default=1.0, help='DDN MIæŸå¤±æƒé‡')

parser.add_argument('--mode', type=str, help='è¿è¡Œæ¨¡å¼')
parser.add_argument('--port', type=int, help='ç«¯å£å·')

args = parser.parse_args()

# æ•°æ®é›†é…ç½®
dataset_config = {
    'PACS': {
        'domains': ['photo', 'art', 'cartoon', 'sketch'],
        'num_classes': 7
    },
    'Office-Home': {
        'domains': ['art', 'clipart', 'product', 'real'],
        'num_classes': 65
    },
    'VLCS': {
        'domains': ['pascal', 'labelme', 'caltech', 'sun'],
        'num_classes': 5
    }
}

# æ•°æ®è½¬æ¢
normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])

train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    normalize,
])

test_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    normalize,
])


# æ•°æ®åŠ è½½å‡½æ•°
def load_data(dataset_name, target_domain_idx):
    config = dataset_config[dataset_name]
    domains = config['domains']
    num_classes = config['num_classes']

    target_domain = domains[target_domain_idx]
    source_domains = [d for d in domains if d != target_domain]

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    source_loaders = []
    for i, domain in enumerate(source_domains):
        domain_path = os.path.join(args.data_root, dataset_name, domain)
        domain_dataset = datasets.ImageFolder(root=domain_path, transform=train_transform)
        domain_loader = DataLoader(
            domain_dataset, batch_size=args.batch_size // len(source_domains),
            shuffle=True, num_workers=4, drop_last=True
        )
        source_loaders.append((i, domain_loader))

    # ç›®æ ‡åŸŸæ•°æ®
    target_path = os.path.join(args.data_root, dataset_name, target_domain)
    target_dataset = datasets.ImageFolder(root=target_path, transform=test_transform)
    target_loader = DataLoader(
        target_dataset, batch_size=args.batch_size,
        shuffle=False, num_workers=4
    )

    return source_loaders, target_loader, num_classes, len(source_domains)


# åˆ›å»ºæ¨¡å‹å‡½æ•°
def create_model(num_classes, num_domains):
    # åˆ›å»ºæ¨¡å‹
    if args.backbone == 'resnet18':
        feature_extractor = models.resnet18(pretrained=True)
        feature_dim = 512
    elif args.backbone == 'resnet50':
        feature_extractor = models.resnet50(pretrained=True)
        feature_dim = 2048
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„backbone: {args.backbone}")

    # ç§»é™¤åˆ†ç±»å±‚
    feature_extractor = nn.Sequential(*list(feature_extractor.children())[:-1])
    feature_extractor.add_module('flatten', nn.Flatten())

    # åˆ›å»ºDÂ²IFLNæ¨¡å‹
    model = D2IFLN(
        feature_extractor=feature_extractor,
        feature_dim=feature_dim,
        num_classes=num_classes,
        num_domains=num_domains,
        hidden_dim=args.hidden_dim
    )

    # è½¬ç§»åˆ°GPU
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    return model, device


# ä¿®æ”¹è®­ç»ƒå‡½æ•°ä»¥æ¥å—ä¼˜åŒ–å™¨å¹¶å®ç°è¿­ä»£è®­ç»ƒ
def train(model, optimizers, source_loaders, num_domains, device, epoch):
    model.train()
    total_loss_log = 0 # ç”¨äºæ—¥å¿—è®°å½•çš„æ€»æŸå¤±

    difln_optimizer = optimizers['difln']
    sdn_stage1_optimizer = optimizers['sdn1']
    sdn_stage2_optimizer = optimizers['sdn2']
    ddn_stage1_optimizer = optimizers['ddn1']
    ddn_stage2_optimizer = optimizers['ddn2']

    ''' 
    å‡è®¾ä½¿ç”¨PACSæ•°æ®é›†ï¼Œç›®æ ‡åŸŸæ˜¯'sketch'ï¼Œé‚£ä¹ˆæºåŸŸä¸º['photo', 'art', 'cartoon']ï¼š
    source_loaders = [
    (0, photo_data_loader),    # i=0, photoåŸŸ
    (1, art_data_loader),      # i=1, artåŸŸ
    (2, cartoon_data_loader)   # i=2, cartoonåŸŸ
    ]
    '''
    # è·å–æ•°æ®è¿­ä»£å™¨
    # iterators = [iter(photo_data_loader), iter(art_data_loader), iter(cartoon_data_loader)]
    iterators = [iter(loader) for _, loader in source_loaders]

    # è®¡ç®—è¿­ä»£æ¬¡æ•°ï¼Œç¡®ä¿æ‰€æœ‰åŸŸçš„æ•°æ®éƒ½è¢«éå†å¤§è‡´ç›¸åŒçš„æ¬¡æ•°
    num_iters = min([len(loader) for _, loader in source_loaders])

    for iter_idx in range(num_iters):
        # æ”¶é›†æ¥è‡ªå„ä¸ªæºåŸŸçš„æ ·æœ¬
        all_x, all_y, all_domains = [], [], []
        for domain_idx, iterator in enumerate(iterators):
            '''
            enumerate(iterators) ä¼šäº§ç”Ÿï¼š
            # (0, iter(photo_data_loader))
            # (1, iter(art_data_loader))
            # (2, iter(cartoon_data_loader))
            '''
            try:
                x_batch, y_batch = next(iterator)
            except StopIteration:
                # å³å½“è¿›å…¥ä¸‹ä¸€ä¸ªepochæ—¶ï¼šæºåŸŸCçš„è¿­ä»£å™¨å·²ç»è€—ã€æºåŸŸAå’ŒBçš„è¿­ä»£å™¨è¿˜æœ‰å‰©ä½™æ•°æ®
                # æºåŸŸCè¿­ä»£å™¨è€—å°½ï¼Œé‡æ–°åˆå§‹åŒ–
                # ğŸŒŸ ä¸è®©ä»»ä½•åŸŸçš„æ•°æ®å›  !!å…¶ä»–åŸŸæ•°æ®å°‘!! è€Œè¢«æµªè´¹ã€‚ç¡®ä¿æ•°æ®å……åˆ†åˆ©ç”¨ï¼š
                iterators[domain_idx] = iter(source_loaders[domain_idx][1])
                x_batch, y_batch = next(iterators[domain_idx])

            x_batch, y_batch = x_batch.to(device), y_batch.to(device)
            all_x.append(x_batch)
            all_y.append(y_batch)
            # ä¸ºæ¯ä¸ªåŸŸçš„æ ·æœ¬åˆ›å»ºåŸŸæ ‡ç­¾
            all_domains.append(torch.full((x_batch.size(0),), domain_idx, dtype=torch.long, device=device))


        # åˆå¹¶æ¥è‡ªæ‰€æœ‰æºåŸŸçš„æ•°æ®
        x = torch.cat(all_x, dim=0)
        y = torch.cat(all_y, dim=0)
        domains = torch.cat(all_domains, dim=0)

        # region  ç®—æ³•æè¿°çš„å†…éƒ¨è¿­ä»£è¿‡ç¨‹
        # æ³¨æ„ï¼šæ¯æ¬¡ backward åè®¡ç®—å›¾ä¼šè¢«é‡Šæ”¾ï¼Œæ‰€ä»¥åœ¨æ¯ä¸ªä¼˜åŒ–æ­¥éª¤å‰éœ€è¦é‡æ–°è®¡ç®—æŸå¤±
        # è¿™å¯èƒ½æ•ˆç‡ä¸é«˜ï¼Œæ›´ä¼˜åŒ–çš„æ–¹æ³•æ˜¯åªè®¡ç®—å½“å‰é˜¶æ®µéœ€è¦çš„æŸå¤±å¹¶åªåå‘ä¼ æ’­è¯¥æŸå¤±ï¼Œ
        # ä½†ä¸ºæ¸…æ™°èµ·è§ï¼Œè¿™é‡Œæ¯æ¬¡éƒ½è°ƒç”¨ compute_losses è·å–æ‰€æœ‰æŸå¤±å€¼ã€‚

        # 1. è®­ç»ƒ DIFLN (æ›´æ–° G, Dc, {F_mlp}, F_gcn, Fd) æŸå¤±: mlp_loss, gcn_loss, cr_loss, adv_loss
        difln_optimizer.zero_grad()
        # å‰å‘ä¼ æ’­ D2IFLN  è·å–æ‰€æœ‰æŸå¤±å€¼
        losses = model.compute_losses(x, y, domains)
        difln_loss = losses['mlp_loss'] + \
                     args.lambda_gcn * losses['gcn_loss'] + \
                     args.lambda_cr * losses['cr_loss'] + \
                     args.lambda_adv * losses['adv_loss'] # å¯¹æŠ—æŸå¤±å½±å“ G, Dc, Fd
        '''PyTorchæ„å»ºçš„è®¡ç®—å›¾åªåŒ…å«å®é™…å‚ä¸è®¡ç®—çš„èŠ‚ç‚¹ã€‚
        åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­æ¢¯åº¦åªä¼šåœ¨å®é™…å‚ä¸è®¡ç®—çš„å‚æ•°ä¹‹é—´æµåŠ¨,æœªå‚ä¸è®¡ç®—çš„å‚æ•°ä¸ä¼š"å¸æ”¶"æˆ–"åˆ†æ•£"æ¢¯åº¦'''
        difln_loss.backward()
        difln_optimizer.step()

        # 2. è®­ç»ƒ SDN - ç¬¬ä¸€é˜¶æ®µ (æ›´æ–° {D_s^k}) æŸå¤±: ent_loss
        sdn_stage1_optimizer.zero_grad()
        # éœ€è¦é‡æ–°è®¡ç®—æŸå¤±ï¼Œå› ä¸ºæ¨¡å‹å‚æ•°å·²æ›´æ–°
        losses = model.compute_losses(x, y, domains)
        sdn_stage1_loss = args.lambda_ent * losses['ent_loss']
        # åªåå‘ä¼ æ’­æ­¤é˜¶æ®µçš„æŸå¤±
        sdn_stage1_loss.backward()
        sdn_stage1_optimizer.step()

        # 3. è®­ç»ƒ SDN - ç¬¬äºŒé˜¶æ®µ (æ›´æ–° {Dc, D_s^k, M_s^k}) æŸå¤±: mis_loss
        sdn_stage2_optimizer.zero_grad()
        losses = model.compute_losses(x, y, domains)
        sdn_stage2_loss = args.lambda_mis * losses['mis_loss']
        sdn_stage2_loss.backward()
        sdn_stage2_optimizer.step()

        # 4. è®­ç»ƒ DDN - ç¬¬ä¸€é˜¶æ®µ (æ›´æ–° {Dd, Fd}) æŸå¤±: dce_loss
        ddn_stage1_optimizer.zero_grad()
        losses = model.compute_losses(x, y, domains)
        ddn_stage1_loss = args.lambda_dce * losses['dce_loss']
        ddn_stage1_loss.backward()
        ddn_stage1_optimizer.step()

        # 5. è®­ç»ƒ DDN - ç¬¬äºŒé˜¶æ®µ (æ›´æ–° {Dc, Dd, Md})  æŸå¤±: mid_loss
        ddn_stage2_optimizer.zero_grad()
        losses = model.compute_losses(x, y, domains)
        ddn_stage2_loss = args.lambda_mid * losses['mid_loss']
        ddn_stage2_loss.backward()
        ddn_stage2_optimizer.step()
        # endregion

        # ç´¯åŠ ç”¨äºæ—¥å¿—è®°å½•çš„æŸå¤±å€¼ï¼ˆä½¿ç”¨æ¯ä¸ªé˜¶æ®µå®é™…åå‘ä¼ æ’­çš„å€¼ï¼‰
        total_loss_log += (difln_loss.item() + sdn_stage1_loss.item() +
                           sdn_stage2_loss.item() + ddn_stage1_loss.item() +
                           ddn_stage2_loss.item())

        # --- å†…éƒ¨è¿­ä»£è¿‡ç¨‹ç»“æŸ ---

    avg_loss = total_loss_log / num_iters
    print(f'Epoch {epoch}, Avg Iter Loss: {avg_loss:.4f}')
    return avg_loss


# æµ‹è¯•å‡½æ•°
def test(model, target_loader, device):
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for x, y in target_loader:
            x, y = x.to(device), y.to(device)
            pred = model.predict(x)

            correct += (pred == y).sum().item()
            total += y.size(0)

    accuracy = 100.0 * correct / total
    print(f'Test Accuracy: {accuracy:.2f}%')

    return accuracy


# ä¿®æ”¹å®Œæ•´çš„è®­ç»ƒå’Œæµ‹è¯•å‡½æ•°
def train_and_evaluate(target_domain_idx):
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)

    # åŠ è½½æ•°æ®
    source_loaders, target_loader, num_classes, num_source_domains = load_data(
        args.dataset, target_domain_idx
    )
    # æ³¨æ„ï¼šæ¨¡å‹åˆå§‹åŒ–éœ€è¦çš„æ˜¯æºåŸŸçš„æ•°é‡
    model, device = create_model(num_classes, num_source_domains)

    #region --- åˆ›å»ºæ‰€æœ‰ä¼˜åŒ–å™¨ ---
    # 1. DIFLNä¼˜åŒ–å™¨
    difln_params = list(model.feature_extractor.parameters()) + \
                   list(model.difln.semantic_disentangler.parameters()) + \
                   list(model.difln.mlp_classifiers.parameters()) + \
                   list(model.difln.edge_network.parameters()) + \
                   list(model.difln.node_classifiers.parameters()) + \
                   list(model.ddn.domain_classifier.parameters()) # Fd åŒ…å«åœ¨å†…
    difln_optimizer = optim.SGD(difln_params, lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)

    # 2. SDNç¬¬ä¸€é˜¶æ®µä¼˜åŒ–å™¨
    sdn_stage1_params = list(model.sdn.domain_specific_disentanglers.parameters())
    sdn_stage1_optimizer = optim.SGD(sdn_stage1_params, lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)

    # 3. SDNç¬¬äºŒé˜¶æ®µä¼˜åŒ–å™¨
    sdn_stage2_params = list(model.difln.semantic_disentangler.parameters()) + \
                        list(model.sdn.domain_specific_disentanglers.parameters()) + \
                        list(model.sdn.mi_estimators.parameters())
    sdn_stage2_optimizer = optim.SGD(sdn_stage2_params, lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)

    # 4. DDNç¬¬ä¸€é˜¶æ®µä¼˜åŒ–å™¨
    ddn_stage1_params = list(model.ddn.domain_disentangler.parameters()) + \
                        list(model.ddn.domain_classifier.parameters())
    ddn_stage1_optimizer = optim.SGD(ddn_stage1_params, lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)

    # 5. DDNç¬¬äºŒé˜¶æ®µä¼˜åŒ–å™¨
    ddn_stage2_params = list(model.difln.semantic_disentangler.parameters()) + \
                        list(model.ddn.domain_disentangler.parameters()) + \
                        list(model.ddn.mi_estimator.parameters())
    ddn_stage2_optimizer = optim.SGD(ddn_stage2_params, lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)

    optimizers = {
        'difln': difln_optimizer,
        'sdn1': sdn_stage1_optimizer,
        'sdn2': sdn_stage2_optimizer,
        'ddn1': ddn_stage1_optimizer,
        'ddn2': ddn_stage2_optimizer
    }
    # endregion --- ä¼˜åŒ–å™¨åˆ›å»ºç»“æŸ ---

    # å­¦ä¹ ç‡è°ƒåº¦å™¨ (åº”ç”¨åˆ°ä¸»ä¼˜åŒ–å™¨ difln_optimizer)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(difln_optimizer, T_max=args.epochs)

    best_acc = 0
    for epoch in range(1, args.epochs + 1):
        # å°†æ‰€æœ‰ä¼˜åŒ–å™¨ä¼ é€’ç»™ train å‡½æ•°
        train_loss = train(model, optimizers, source_loaders, num_source_domains, device, epoch)
        # å¯¹ä¸»ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡è¿›è¡Œè°ƒæ•´
        scheduler.step()
        current_lr = scheduler.get_last_lr()[0] # è·å–å½“å‰å­¦ä¹ ç‡
        # å¯é€‰ï¼šå°†å…¶ä»–ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡ä¹ŸåŒæ­¥è°ƒæ•´ï¼ˆå¦‚æœéœ€è¦ï¼‰
        for name, opt in optimizers.items():
             if name != 'difln':
                 for param_group in opt.param_groups:
                     param_group['lr'] = current_lr
        print(f"Epoch {epoch}, Current LR: {current_lr:.6f}")


        # æ¯ä¸ªepochç»“æŸåè¿›è¡Œæµ‹è¯•
        test_acc = test(model, target_loader, device)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if test_acc > best_acc:
            best_acc = test_acc
            # ç¡®ä¿ä¿å­˜è·¯å¾„å­˜åœ¨
            os.makedirs(os.path.dirname(f'./checkpoints/best_model_{args.dataset}_target_{target_domain_idx}.pth'), exist_ok=True)
            torch.save(model.state_dict(), f'./checkpoints/best_model_{args.dataset}_target_{target_domain_idx}.pth')


    print(f'ç›®æ ‡åŸŸ {target_domain_idx} ({dataset_config[args.dataset]["domains"][target_domain_idx]}) çš„æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_acc:.2f}%')
    return best_acc


if __name__ == "__main__":
    print(f"è®­ç»ƒæ•°æ®é›†: {args.dataset}")
    print(f"backbone: {args.backbone}")
    print(f"å‚æ•°: {args}") # æ‰“å°æ‰€æœ‰å‚æ•°

    # è·å–æ•°æ®é›†çš„æ‰€æœ‰åŸŸ
    domains = dataset_config[args.dataset]['domains']
    num_total_domains = len(domains) # æ€»åŸŸæ•°

    # å®ç°å®Œæ•´çš„leave-one-domain-outç­–ç•¥
    all_accuracies = []

    # éå†æ¯ä¸ªåŸŸä½œä¸ºç›®æ ‡åŸŸ
    for target_idx in range(num_total_domains):
        print(f"\n{'=' * 50}")
        print(f"ç›®æ ‡åŸŸ: {domains[target_idx]} (ç´¢å¼• {target_idx})")
        source_domain_names = [domains[i] for i in range(num_total_domains) if i != target_idx]
        print(f"æºåŸŸ: {source_domain_names}")
        print(f"{'=' * 50}\n")

        # è®­ç»ƒå¹¶è¯„ä¼°æ¨¡å‹
        accuracy = train_and_evaluate(target_idx)
        all_accuracies.append(accuracy)

    # è®¡ç®—å¹¶è¾“å‡ºå¹³å‡å‡†ç¡®ç‡
    avg_accuracy = sum(all_accuracies) / len(all_accuracies) if all_accuracies else 0
    print(f"\n{'=' * 70}")
    print(f"æ‰€æœ‰åŸŸä½œä¸ºç›®æ ‡åŸŸçš„å‡†ç¡®ç‡: {all_accuracies}")
    print(f"å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.2f}%")
    print(f"{'=' * 70}")

    # å°†ç»“æœä¿å­˜åˆ°æ–‡ä»¶
    results_dir = "./results"
    os.makedirs(results_dir, exist_ok=True)
    results_filename = os.path.join(results_dir, f"{args.dataset}_{args.backbone}_results.txt")

    with open(results_filename, "w") as f:
        f.write(f"æ•°æ®é›†: {args.dataset}\n")
        f.write(f"Backbone: {args.backbone}\n")
        f.write(f"å‚æ•°: {args}\n\n")

        for i, domain in enumerate(domains):
            f.write(f"ç›®æ ‡åŸŸ {domain}: {all_accuracies[i]:.2f}%\n")

        f.write(f"\nå¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.2f}%\n")

    print(f"ç»“æœå·²ä¿å­˜åˆ°: {results_filename}")